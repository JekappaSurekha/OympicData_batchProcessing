# OlympicData-DE-Project1
Data_Engineering_Project1

#BatchProcessing
This is the Olympic-Data-Engineering Project from End to End . This Project involves step by step pipeline. 
Flow of the Project :
1. Getting the data from Kaggle (CSV) into our local storage
2. Uploading the data to cloud by ADF(data ingestion is a pipeline task)
3. Storing that data in the data lake
4. Doing Some Transformations by using Data Bricks
5. Again Storing the Transformed Data into DataLake
6. Performing some Analytics by using Azure Synapse analytics
7. At the end connecting to Dashboard( PowerBI or Tableau)
